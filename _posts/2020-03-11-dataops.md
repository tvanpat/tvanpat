---
title: "You Have Heard of DevOps and SecDevOps, Well Now It Is Time for DataOps!"
date: 2020-03-11
tags: [Data Science, DataOps, Data Architecture, ML Architecture]
excerpt: "How to move your ML project into a sustainable Ops style environment"
---

While I was getting my Masters a question was proposed to our professor, "How do we take the models
we are building and put them into a production environment?"  The answer was a bit underwhelming.  
We were told that we would simply hand over the model to a developer who would refactor our messy
code and place it into production.  Well I am calling shenanigans.  I mean I am sure there is a company
somewhere where there are willing developers to take my code, clean it, and build a data pipeline around
it, but I have a feeling this the exception and not the norm.

My second problem with this statement is it treats models as fire and forget.  While a data scientist
can build a model and send it off into the wild and move on to the next project, it is probably not a
good idea.  This is because at its most base form a model is just a simplification of reality and it is
build on assumptions.  If our assumptions are wrong the model will be wrong.  Once the model is
in production it needs to be monitored to ensure the assumptions made while building the model
hold true, and if not the model needs to be adjusted.  Moreover, the world is not a static environment
nor should our models be.  For a model to be relevant it needs to be adjusted and updated.  In a fire
and forget model deployment the model is never adjusted or updated.

We need a method where models can provide quick predictions, these predictions and raw data are logged,
model drift can be monitored, and new models can be tested and when ready integrated.  We need
**DATAOPS**.  An example of a DataOps architecture is shown below.  Admittedly most of this is
not my idea and was presented in the book [The Rendezvous Architecture for Machine Learning](https://mapr.com/ebooks/machine-learning-logistics/ch03.html) by Ted Dunning and Ellen Friedman.

![Data Ops]({{site.url}}/images/blog/2020-03-11/dataops.png)

- We start with a request.  I envision that as part of a larger ETL process an REST API request is sent
to our model.  

- Once the model receives the request the data preprocessing and data enrichment is completed.  This
can either be done in the application or another REST API request can be sent out.  To see what method
works best testing would be required.

- Now the data is in the proper format for the model three things will happen.
